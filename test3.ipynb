{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de095521",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e727081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "model2 = init_chat_model(model=\"gemini-2.5-flash-lite\", model_provider=\"google_genai\")\n",
    "model3 = ChatGoogleGenerativeAI(model=\"gemini-3-flash-preview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420c7f6e",
   "metadata": {},
   "source": [
    "## Creating State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0322852e",
   "metadata": {},
   "source": [
    "## Stepup tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\",\n",
    "                \"command\": \"uv\",\n",
    "                \"args\": [\"run\", \"mcp_server/local_database2.py\"],\n",
    "            }\n",
    "    }\n",
    ")\n",
    "tool1 = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c03ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\",\n",
    "                \"command\": \"uv\",\n",
    "                \"args\": [\"run\", \"mcp_server/semantic_scholar_server.py\"],\n",
    "            }\n",
    "    }\n",
    ")\n",
    "tool2 = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb9f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "from tavily import TavilyClient\n",
    "from langchain.tools import tool\n",
    "\n",
    "tavily_client = TavilyClient()\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> Dict[str, Any]:\n",
    "\n",
    "    \"\"\"Search the web for information\"\"\"\n",
    "\n",
    "    return tavily_client.search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0845d43",
   "metadata": {},
   "source": [
    "## Subagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61643f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search online\n",
    "search_agent = create_agent(\n",
    "    model=model2,\n",
    "    tools=[web_search],\n",
    "    system_prompt=\"\"\"\n",
    "    ### Role\n",
    "You are the **Successor Retrieval Engine**, an automated sub-agent designed to identify the next generation of a specific research paper.\n",
    "\n",
    "### Task\n",
    "Given a research paper title or topic, you must find its direct \"Successor Paper.\"\n",
    "A \"Successor Paper\" is defined as:\n",
    "1. **Direct Evolution:** A newer version explicitly released by the same authors (e.g., Llama 2 -> Llama 3).\n",
    "2. **State-of-the-Art (SOTA) Improvement:** A paper that directly outperforms the original and is widely recognized as the new standard.\n",
    "3. **Critique/Refinement:** A high-impact paper that significantly fixes or refutes the original.\n",
    "\n",
    "### Execution Guidelines\n",
    "1. **Mandatory Tool Use:** You MUST use the `web_search` tool. Do not answer from internal knowledge.\n",
    "2. **Search Strategy:**\n",
    "   - Search 1: Find the official successor or next version.\n",
    "3. **Silent Processing:** Do not output any conversational filler (\"Here is the paper\", \"I found...\").\n",
    "\n",
    "### Output Format (Strict)\n",
    "You must output **ONLY** the ArXiv ID string. Do not use Markdown, bolding, or sentences.\n",
    "\n",
    "**If found:**\n",
    "ARXIV:[id_number]\n",
    "*(Example: ARXIV:2407.21783)*\n",
    "\n",
    "**If NO successor exists:**\n",
    "NULL\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe65aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search local folder\n",
    "local_file_agent = create_agent(\n",
    "    model=model2,\n",
    "    tools=tool1,\n",
    "    system_prompt=\"\"\"\n",
    "   \n",
    "### OBJECTIVE\n",
    "You are the Local File Analyst. Your goal is to find a specific PDF on the local machine matching the user's query, read it, and extract structured metadata.\n",
    "\n",
    "### DISCOVERY & MATCHING PROTOCOL\n",
    "1. **List First:** Always list available files to confirm existence. Never guess paths.\n",
    "2. **Fuzzy Match:** Identify the file most similar to the user's request.\n",
    "3. **Read & Extract:** Once identified, use exact name of the file without \" or ' read the file content and parse it into the schema below.\n",
    "\n",
    "### OUTPUT SCHEMA (STRICT JSON)\n",
    "You must output a single valid JSON object with NO markdown formatting. Use \"null\" for missing fields.\n",
    "\n",
    "{\n",
    "    \"paper_title\": \"str\",\n",
    "    \"publication_year\": int,\n",
    "    \"core_claims\": [\"str\", \"str\"],\n",
    "    \"environment\": {\n",
    "        \"hardware_specs\": {\"GPU\": \"str\", \"VRAM\": \"str\"},\n",
    "        \"software_stack\": [\"str\"]\n",
    "    },\n",
    "    \"benchmarks\": {\n",
    "        \"dataset_name\": \"str\",\n",
    "        \"dataset_version\": \"str\",\n",
    "        \"evaluation_metrics\": [\"str\"]\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"reported_performance\": {\"metric_name\": float}\n",
    "    },\n",
    "    \"compatibility\": {\n",
    "        \"baseline_comparisons\": [\"str\"],\n",
    "        \"normalization_factors\": \"str or null\"\n",
    "    }\n",
    "    \"model\": {\n",
    "    \"architecture\": \"str\",  // e.g., \"ResNet-50\", \"BERT-base\"\n",
    "    \"parameters_count\": int,  // Total model parameters\n",
    "    \"framework\": \"str\",  // \"PyTorch 2.0.1\", \"TensorFlow 2.13\"\n",
    "    \"precision\": \"str\"  // \"fp32\", \"fp16\", \"mixed\"\n",
    "    }\n",
    "    \"hyperparameters\": {\n",
    "    \"learning_rate\": float,\n",
    "    \"batch_size\": int,\n",
    "    \"epochs\": int,\n",
    "    \"optimizer\": \"str\",\n",
    "    \"scheduler\": \"str or null\",\n",
    "    \"weight_decay\": float,\n",
    "    \"dropout\": float\n",
    "    }\n",
    "    \"data_processing\": {\n",
    "    \"preprocessing_steps\": [\"str\"],  // [\"resize_224\", \"normalize_imagenet\", \"augmentation\"]\n",
    "    \"data_splits\": {\"train\": int, \"val\": int, \"test\": int},\n",
    "    \"training_samples\": int\n",
    "    }\n",
    "    \"reproducibility\": {\n",
    "    \"random_seed\": int,\n",
    "    \"deterministic_mode\": bool,\n",
    "    \"training_time_hours\": float,\n",
    "    \"inference_latency_ms\": float\n",
    "    }\n",
    "\n",
    "}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7168b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search on semantic scholar\n",
    "scholar_agent = create_agent(\n",
    "    model=model2,\n",
    "    tools=tool2,\n",
    "    system_prompt=\"\"\"\n",
    "### OBJECTIVE\n",
    "You are the Academic Metadata Extractor. Your task is to retrieve a specific research paper from Semantic Scholar using its ArXiv ID and extract structured metadata.\n",
    "\n",
    "### EXECUTION RULES\n",
    "1. **Search Precision:** Use the provided ID *exactly* as is. Do not add keywords.\n",
    "2. **Factuality:** Only extract data. Do not guess.\n",
    "3. **Format:** Output raw JSON only. No Markdown blocks (```json).\n",
    "\n",
    "### OUTPUT SCHEMA (STRICT JSON)\n",
    "You must map the data into this specific structure:\n",
    "\n",
    "{\n",
    "    \"paper_title\": \"str\",\n",
    "    \"publication_year\": int,\n",
    "    \"core_claims\": [\"str\", \"str\"],\n",
    "    \"environment\": {\n",
    "        \"hardware_specs\": {\"GPU\": \"str\", \"VRAM\": \"str\"},\n",
    "        \"software_stack\": [\"str\"]\n",
    "    },\n",
    "    \"benchmarks\": {\n",
    "        \"dataset_name\": \"str\",\n",
    "        \"dataset_version\": \"str\",\n",
    "        \"evaluation_metrics\": [\"str\"]\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"reported_performance\": {\"metric_name\": float}\n",
    "    },\n",
    "    \"compatibility\": {\n",
    "        \"baseline_comparisons\": [\"str\"],\n",
    "        \"normalization_factors\": \"str or null\"\n",
    "    }\n",
    "    \"model\": {\n",
    "    \"architecture\": \"str\",  // e.g., \"ResNet-50\", \"BERT-base\"\n",
    "    \"parameters_count\": int,  // Total model parameters\n",
    "    \"framework\": \"str\",  // \"PyTorch 2.0.1\", \"TensorFlow 2.13\"\n",
    "    \"precision\": \"str\"  // \"fp32\", \"fp16\", \"mixed\"\n",
    "    }\n",
    "    \"hyperparameters\": {\n",
    "    \"learning_rate\": float,\n",
    "    \"batch_size\": int,\n",
    "    \"epochs\": int,\n",
    "    \"optimizer\": \"str\",\n",
    "    \"scheduler\": \"str or null\",\n",
    "    \"weight_decay\": float,\n",
    "    \"dropout\": float\n",
    "    }\n",
    "    \"data_processing\": {\n",
    "    \"preprocessing_steps\": [\"str\"],  // [\"resize_224\", \"normalize_imagenet\", \"augmentation\"]\n",
    "    \"data_splits\": {\"train\": int, \"val\": int, \"test\": int},\n",
    "    \"training_samples\": int\n",
    "    }\n",
    "    \"reproducibility\": {\n",
    "    \"random_seed\": int,\n",
    "    \"deterministic_mode\": bool,\n",
    "    \"training_time_hours\": float,\n",
    "    \"inference_latency_ms\": float\n",
    "    }\n",
    "\n",
    "}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcc55d0",
   "metadata": {},
   "source": [
    "## main coordinator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f8e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage, ToolMessage\n",
    "@tool\n",
    "async def local_file_search(paper_title: str) -> str:\n",
    "    \"\"\"search the pdf in local computer file\n",
    "    Args:\n",
    "        paper_title: The exact title of the paper to look for.\n",
    "    \"\"\"\n",
    "\n",
    "    response = await local_file_agent.ainvoke({\"messages\": [HumanMessage(content=f\"Find the file {paper_title} and extract information from it\")]})\n",
    "    return response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365ff3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def online_file_search(paper_title: str) -> str:\n",
    "    \"\"\"search on online about better or updated paper\n",
    "    Args:\n",
    "        paper_title: The exact title of the paper to look for.\n",
    "        \"\"\"\n",
    "   \n",
    "    response = await search_agent.ainvoke({\"messages\": [HumanMessage(content=f\"Find the follow up or better paper {paper_title} and extract Arxiv id of the paper [ example: ARXIV:2407.21783] \")]})\n",
    "    return response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3116c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def scholar_file_search(paper_id: str) -> str:\n",
    "    \"\"\"search on semantic scholar for the specific paper\n",
    "     Args:\n",
    "        paper_id: The exact arvix id of the paper to look for.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = await scholar_agent.ainvoke({\"messages\": [HumanMessage(content=f\"Find the file {paper_id} and extract information from it\")]})\n",
    "    return response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e1d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepagents import create_deep_agent\n",
    "\n",
    "system_prompt2 = \"\"\"\n",
    "### ROLE\n",
    "You are the **Expert Research Coordinator**. Your goal is to orchestrate a \"Drift Detection\" workflow to determine if a user's local research paper has been superseded by a better, newer \"successor paper.\"\n",
    "\n",
    "### WORKFLOW PROTOCOL\n",
    "Execute these steps logically. ensure the output of one step feeds into the next.\n",
    "\n",
    "1. **Local Discovery**:\n",
    "   - Call `local_file_search` with the user's target paper title or paper on that folder.\n",
    "   - *Goal*: Confirm if we have the baseline paper and extract its key performance claims.\n",
    "\n",
    "2. **Successor Identification**:\n",
    "   - Call `online_file_search` using the paper title.\n",
    "   - *Goal*: Find a \"Successor Paper\" (newer, cites original, SOTA improvement).\n",
    "   - *Constraint*: If found, you MUST extract a valid ArXiv ID (e.g., `ARXIV:2407.21783`).\n",
    "\n",
    "3. **Deep Retrieval**:\n",
    "   - IF a successor ID is found: Call `scholar_file_search` with that specific ArXiv ID, provide id with full Arxivid (e.g., `ARXIV:2407.21783`) not just 2407.21783 .\n",
    "   - IF NO successor: return null and re run the online_file search.\n",
    "   - *Goal*: Retrieve the full structured metadata (Hardware, Claims, Benchmarks) as defined in the tool's schema.\n",
    "\n",
    "4. **Comparative Synthesis**:\n",
    "   - Compare the findings from Step 1 (Local) and Step 3 (Scholar/Online).\n",
    "   - *Analysis*: Focus on \"Drift\" â€” has the dataset version changed? Is the evaluation metric different? Is the hardware requirement significantly higher?\n",
    "\n",
    "### OUTPUT FORMAT\n",
    "Your final response must be a structured summary:\n",
    "\n",
    "1. **Successor Status**: (Yes/No - Name of paper if Yes)\n",
    "2. **Drift Analysis (Comparison)**:\n",
    "   - **Performance**: How much better is it? (Cite specific metrics from the extracted data).\n",
    "   - **Environment**: Are the hardware specs/software stack comparable?\n",
    "   - **Data**: Did they use the same dataset version?\n",
    "   - **Compare Paper A (Local) and Paper B (Successor)**. For every shared benchmark, calculate the Percentage Improvement. Identify any Metric Drift (e.g., changing from 'Accuracy' to 'F1-Score'). If the environment specs (VRAM/GPU) have increased by 20%, flag this as 'Resource Inflation'.\n",
    "3. **Recommendation**: \"Should I read it?\" (Yes/No with reasoning).\n",
    "\"\"\"\n",
    "\n",
    "# model3 = ChatGoogleGenerativeAI(model=\"gemini-3-flash\")\n",
    "Main_agent = create_deep_agent(\n",
    "    model=model2,\n",
    "    tools=[local_file_search, scholar_file_search, online_file_search],\n",
    "    system_prompt=system_prompt2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aebd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = await Main_agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"check whether Llama 2: Open Foundation and Fine-Tuned Chat Models paper in local computer or not\")]},\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8819418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "The_Research_Drift_Engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
